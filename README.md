# llama.cpp-simple-api-client
The repository contains a client script in Python for interacting with the LLM model via an HTTP API llama-server. The code allows sending user messages and receiving responses from the model using various API routes (e.g. `/completions` or `/v1/chat/completions`). Streaming data is also supported.
